import streamlit as st
import torch
import pickle
import os
import requests
from model import GPTConfig, GPT

# --- CONFIGURATION DE LA PAGE & THÃˆME ---
st.set_page_config(page_title="UiT nanoGPT Storyteller", page_icon="ðŸŸ¢")

# Style CSS pour le thÃ¨me Light Green (ChatGPT Style)
st.markdown("""
    <style>
    .stApp { background-color: #f0fdf4; } 
    [data-testid="stSidebar"] { background-color: #dcfce7; }
    .stChatMessage[data-testid="stChatMessageUser"] { background-color: #bbf7d0; border-radius: 15px; }
    .stChatMessage[data-testid="stChatMessageAssistant"] { background-color: #ffffff; border-radius: 15px; border: 1px solid #e2e8f0; }
    .stButton>button { background-color: #22c55e; color: white; border-radius: 20px; border: none; }
    </style>
    """, unsafe_allow_html=True)

# --- CONFIGURATION DU MODÃˆLE ---
FILE_ID = '1rLJSJQwdvRhRS8KdYjffTM-jkhPM0zGr'
CKPT_PATH = 'ckpt.pt'

@st.cache_resource
def download_and_load_model():
    if not os.path.exists(CKPT_PATH):
        with st.spinner("RÃ©cupÃ©ration du modÃ¨le (218 Mo)... Google Drive demande une confirmation antivirus."):
            URL = "https://docs.google.com/uc?export=download"
            session = requests.Session()
            # PremiÃ¨re requÃªte pour obtenir le cookie de confirmation
            response = session.get(URL, params={'id': FILE_ID}, stream=True)
            
            token = None
            for key, value in response.cookies.items():
                if key.startswith('download_warning'):
                    token = value
                    break
            
            # Si un jeton est trouvÃ©, on relance la requÃªte avec la confirmation
            if token:
                response = session.get(URL, params={'id': FILE_ID, 'confirm': token}, stream=True)
            
            # Ã‰criture du fichier par morceaux (chunks)
            with open(CKPT_PATH, "wb") as f:
                for chunk in response.iter_content(32768):
                    if chunk: f.write(chunk)

    # VÃ©rification : si le fichier est corrompu ou incomplet
    if os.path.getsize(CKPT_PATH) < 100000000: # Doit faire plus de 100Mo
        st.error("âŒ Le tÃ©lÃ©chargement a Ã©chouÃ©. Google Drive bloque peut-Ãªtre l'accÃ¨s.")
        if os.path.exists(CKPT_PATH): os.remove(CKPT_PATH)
        return None

    try:
        checkpoint = torch.load(CKPT_PATH, map_location='cpu')
        config = GPTConfig(**checkpoint['model_args'])
        model = GPT(config)
        state_dict = checkpoint['model']
        # Nettoyage automatique des prÃ©fixes 'compile'
        state_dict = {k.replace('_orig_mod.', ''): v for k, v in state_dict.items()}
        model.load_state_dict(state_dict)
        model.eval()
        return model
    except Exception as e:
        st.error(f"Erreur lors du chargement des poids : {e}")
        return None

# --- CHARGEMENT DES COMPOSANTS ---
model = download_and_load_model()

if model is None:
    st.warning("âš ï¸ L'application ne peut pas dÃ©marrer sans le modÃ¨le. RafraÃ®chissez la page.")
    st.stop()

# Chargement du dictionnaire meta.pkl (doit Ãªtre sur ton GitHub)
try:
    with open('meta.pkl', 'rb') as f:
        meta = pickle.load(f)
    stoi, itos = meta['stoi'], meta['itos']
    encode = lambda s: [stoi[c] for c in s if c in stoi]
    decode = lambda l: ''.join([itos[i] for i in l])
except FileNotFoundError:
    st.error("âŒ Fichier 'meta.pkl' introuvable dans le dÃ©pÃ´t GitHub.")
    st.stop()

# --- INTERFACE UTILISATEUR ---
st.title("ðŸŸ¢ UiT nanoGPT Storyteller")
st.caption("Architecture 24-layers entraÃ®nÃ©e sur TinyStories par Arthur PREVEL")

if "messages" not in st.session_state:
    st.session_state.messages = []

# Sidebar pour les rÃ©glages
with st.sidebar:
    st.header("ParamÃ¨tres")
    temp = st.slider("CrÃ©ativitÃ© (TempÃ©rature)", 0.1, 1.2, 0.8)
    max_t = st.slider("Longueur de l'histoire", 50, 500, 200)
    if st.button("Nouvelle discussion"):
        st.session_state.messages = []
        st.rerun()

# Affichage des messages
for msg in st.session_state.messages:
    with st.chat_message(msg["role"]):
        st.markdown(msg["content"])

# EntrÃ©e du prompt
if prompt := st.chat_input("Il Ã©tait une fois..."):
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    with st.chat_message("assistant"):
        with st.spinner("GÃ©nÃ©ration en cours..."):
            # Encodage et gÃ©nÃ©ration par le modÃ¨le
            context_ids = torch.tensor(encode(prompt), dtype=torch.long)[None, ...]
            # Appel de ta fonction de gÃ©nÃ©ration nanoGPT
            output_ids = model.generate(context_ids, max_new_tokens=max_t, temperature=temp)[0].tolist()
            response = decode(output_ids)
            st.markdown(response)
    
    st.session_state.messages.append({"role": "assistant", "content": response})
